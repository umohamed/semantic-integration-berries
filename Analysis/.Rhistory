arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
scoring = read_csv("association_scoring.csv")%>%
arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
View(congruence_trials)
scoring = read_csv("association_scoring.csv")%>%
arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
## NEED TO WORK AROUND THE MIPP AND GECK RESPONSES SINCE THEY AREN'T ASSOCIATED WITH A TRIAD
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
congruence_counts = congruence_trials %>%
group_by(ID, cue_type, congruence, type_of_association) %>%
count() %>%
group_by(ID, cue_type) %>%
mutate(proportion = n / sum(n))
congruence_counts %>%
filter(congruence == "congruent") %>%
ungroup()%>%
summarise(mean_prop = mean(proportion))
wide_counts = congruence_counts %>%
select(ID, cue_type, congruence, type_of_association, proportion) %>%
pivot_wider(names_from = congruence, values_from = proportion) %>%
mutate(incongruent = ifelse(is.na(incongruent), 0, incongruent),
congruent = ifelse(is.na(congruent), 0, congruent)) %>%
mutate(prop = congruent - incongruent)
mean(wide_counts$prop)
## counts by type of association
association_type_occurrence = wide_counts %>%
select(ID, cue_type, type_of_association, prop) %>%
pivot_wider(names_from = type_of_association, values_from = prop) %>%
mutate(shared = ifelse(is.na(shared), 0, shared),
direct = ifelse(is.na(direct), 0, direct))
mean(association_type_occurrence$direct)
mean(association_type_occurrence$shared)
# Chunk 1
install.packages("tidyverse")
install.packages("emmeans")
install.packages("performance")
install.packages("see")
install.packages("patchwork")
install.packages("dplyr")
install.packages("tidyr")
# Chunk 2
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
# Chunk 3
#prolific_data = read.csv("")
#sona_data = read.csv("")
# Chunk 4
library(dplyr)
pilot_data = read.csv("first_sona_data.csv") %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
revised_correct = as.numeric(revised_correct))
# Chunk 5
nrow(pilot_data)
levels(pilot_data$relatedness)
# Chunk 6
pilot_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
# Chunk 7
pilot_data %>%
filter(typeoftrial == "target") %>%
pull(rt)
# Chunk 8
pilot_data %>%
pull(ID) %>% unique() %>% length()
# Chunk 9
attention_trials = pilot_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 10
priming_data = pilot_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & type %in% c("direct", "novel")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 11
priming_data %>%
group_by(type, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= coocurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 2
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
# Chunk 3
#prolific_data = read.csv("")
#sona_data = read.csv("")
# Chunk 4
library(dplyr)
pilot_data = read.csv("sona_data_2.csv") %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 5
nrow(pilot_data)
levels(pilot_data$relatedness)
levels(pilot_data$cooccurrence)
# Chunk 6
pilot_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
# Chunk 7
pilot_data %>%
filter(typeoftrial == "target") %>%
pull(rt)
# Chunk 8
pilot_data %>%
pull(ID) %>% unique() %>% length()
# Chunk 9
attention_trials = pilot_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 10
priming_data = pilot_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 11
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 12
scoring = read_csv("association_scoring.csv")%>%
arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
## NEED TO WORK AROUND THE MIPP AND GECK RESPONSES SINCE THEY AREN'T ASSOCIATED WITH A TRIAD
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
congruence_counts = congruence_trials %>%
group_by(ID, cue_type, congruence, type_of_association) %>%
count() %>%
group_by(ID, cue_type) %>%
mutate(proportion = n / sum(n))
congruence_counts %>%
filter(congruence == "congruent") %>%
ungroup()%>%
summarise(mean_prop = mean(proportion))
wide_counts = congruence_counts %>%
select(ID, cue_type, congruence, type_of_association, proportion) %>%
pivot_wider(names_from = congruence, values_from = proportion) %>%
mutate(incongruent = ifelse(is.na(incongruent), 0, incongruent),
congruent = ifelse(is.na(congruent), 0, congruent)) %>%
mutate(prop = congruent - incongruent)
mean(wide_counts$prop)
## counts by type of association
association_type_occurrence = wide_counts %>%
select(ID, cue_type, type_of_association, prop) %>%
pivot_wider(names_from = type_of_association, values_from = prop) %>%
mutate(shared = ifelse(is.na(shared), 0, shared),
direct = ifelse(is.na(direct), 0, direct))
mean(association_type_occurrence$direct)
mean(association_type_occurrence$shared)
# Chunk 13
rt_model = lmer(data = priming_data,
rt ~ relatedness*type + (1|ID))
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 1
install.packages("tidyverse")
install.packages("emmeans")
install.packages("performance")
install.packages("see")
install.packages("patchwork")
install.packages("dplyr")
install.packages("tidyr")
# Chunk 2
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
# Chunk 3
#prolific_data = read.csv("")
#sona_data = read.csv("")
# Chunk 4
library(dplyr)
pilot_data = read.csv("sona_data_2.csv") %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 5
nrow(pilot_data)
levels(pilot_data$relatedness)
levels(pilot_data$cooccurrence)
# Chunk 6
pilot_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
# Chunk 7
pilot_data %>%
filter(typeoftrial == "target") %>%
pull(rt)
# Chunk 8
pilot_data %>%
pull(ID) %>% unique() %>% length()
# Chunk 9
attention_trials = pilot_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 10
priming_data = pilot_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 11
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 12
scoring = read_csv("association_scoring.csv")%>%
arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
## NEED TO WORK AROUND THE MIPP AND GECK RESPONSES SINCE THEY AREN'T ASSOCIATED WITH A TRIAD
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
congruence_counts = congruence_trials %>%
group_by(ID, cue_type, congruence, type_of_association) %>%
count() %>%
group_by(ID, cue_type) %>%
mutate(proportion = n / sum(n))
congruence_counts %>%
filter(congruence == "congruent") %>%
ungroup()%>%
summarise(mean_prop = mean(proportion))
wide_counts = congruence_counts %>%
select(ID, cue_type, congruence, type_of_association, proportion) %>%
pivot_wider(names_from = congruence, values_from = proportion) %>%
mutate(incongruent = ifelse(is.na(incongruent), 0, incongruent),
congruent = ifelse(is.na(congruent), 0, congruent)) %>%
mutate(prop = congruent - incongruent)
mean(wide_counts$prop)
## counts by type of association
association_type_occurrence = wide_counts %>%
select(ID, cue_type, type_of_association, prop) %>%
pivot_wider(names_from = type_of_association, values_from = prop) %>%
mutate(shared = ifelse(is.na(shared), 0, shared),
direct = ifelse(is.na(direct), 0, direct))
mean(association_type_occurrence$direct)
mean(association_type_occurrence$shared)
# Chunk 13
rt_model = lmer(data = priming_data,
rt ~ relatedness*type + (1|ID))
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
# Chunk 1
install.packages("tidyverse")
install.packages("emmeans")
install.packages("performance")
install.packages("see")
install.packages("patchwork")
install.packages("dplyr")
install.packages("tidyr")
# Chunk 2
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 3
#prolific_data = read.csv("")
#sona_data = read.csv("")
# Chunk 4
library(dplyr)
pilot_data = read.csv("sona_data_2.csv") %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 5
nrow(pilot_data)
levels(pilot_data$relatedness)
levels(pilot_data$cooccurrence)
# Chunk 6
pilot_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
# Chunk 7
pilot_data %>%
filter(typeoftrial == "target") %>%
pull(rt)
# Chunk 8
pilot_data %>%
pull(ID) %>% unique() %>% length()
# Chunk 9
attention_trials = pilot_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 10
priming_data = pilot_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 11
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 12
scoring = read_csv("association_scoring.csv")%>%
arrange(cue, response)
association_trials = pilot_data %>%
filter(typeoftrial == "association") %>%
select(ID, revised_response, cue) %>%
rename(response = "revised_response") %>%
mutate(response= tolower(response)) %>%
left_join(scoring)
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
## NEED TO WORK AROUND THE MIPP AND GECK RESPONSES SINCE THEY AREN'T ASSOCIATED WITH A TRIAD
congruence_trials = association_trials %>%
filter(!is.na(congruence)) %>%
filter(congruence %in% c("congruent", "incongruent")) %>%
filter(type_of_association %in% c("direct", "shared"))
congruence_counts = congruence_trials %>%
group_by(ID, cue_type, congruence, type_of_association) %>%
count() %>%
group_by(ID, cue_type) %>%
mutate(proportion = n / sum(n))
congruence_counts %>%
filter(congruence == "congruent") %>%
ungroup()%>%
summarise(mean_prop = mean(proportion))
wide_counts = congruence_counts %>%
select(ID, cue_type, congruence, type_of_association, proportion) %>%
pivot_wider(names_from = congruence, values_from = proportion) %>%
mutate(incongruent = ifelse(is.na(incongruent), 0, incongruent),
congruent = ifelse(is.na(congruent), 0, congruent)) %>%
mutate(prop = congruent - incongruent)
mean(wide_counts$prop)
## counts by type of association
association_type_occurrence = wide_counts %>%
select(ID, cue_type, type_of_association, prop) %>%
pivot_wider(names_from = type_of_association, values_from = prop) %>%
mutate(shared = ifelse(is.na(shared), 0, shared),
direct = ifelse(is.na(direct), 0, direct))
mean(association_type_occurrence$direct)
mean(association_type_occurrence$shared)
# Chunk 13
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
# Chunk 14
nrow(pilot_data)
ncol(pilot_data)
pilot_data  %>% pull(subject)  %>%  unique()
install.packages("tidyverse")
