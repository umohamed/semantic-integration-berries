subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Chunk 6
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 7
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
# Chunk 8
#Research Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
# Chunk 9
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
# Chunk 3
library(dplyr)
final_data = read.csv("sona_data_2.csv") %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
cleaned_data = final_data %>%
filter(relatedness != "") %>%
filter(cooccurrence != "")
levels(cleaned_data$relatedness)
levels(cleaned_data$cooccurrence)
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
# Chunk 7
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 8
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 9
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
# Chunk 3
library(dplyr)
final_data = sona_data_1 %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
cleaned_data = final_data %>%
filter(relatedness != "") %>%
filter(cooccurrence != "")
levels(cleaned_data$relatedness)
levels(cleaned_data$cooccurrence)
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
# Chunk 7
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 8
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 9
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data = rbind(sona_data_1, sona_data_2)
View(sona_data)
View(sona_data_1)
View(sona_data_2)
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data = rbind(sona_data_1, sona_data_2)
# Chunk 3
library(dplyr)
final_data = sona_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
cleaned_data = final_data %>%
filter(relatedness != "") %>%
filter(cooccurrence != "")
levels(cleaned_data$relatedness)
levels(cleaned_data$cooccurrence)
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
# Chunk 7
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 8
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 9
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data = rbind(sona_data_1, sona_data_2)
# Chunk 3
library(dplyr)
final_data = sona_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
cleaned_data = final_data %>%
filter(relatedness != "") %>%
filter(cooccurrence != "")
levels(cleaned_data$relatedness)
levels(cleaned_data$cooccurrence)
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
ggplot(data = cleaned_data, aes(x = cooccurrence, y = rt, fill = relatedness)) +
geom_col(position = "dodge") +
labs(title = "Reaction Time by Cooccurrence and Relatedness",
x = "Cooccurrence",
y = "Reaction Time",
fill = "Relatedness")
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
# Chunk 7
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 8
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 9
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
