final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
#final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
View(final_data)
View(pilot_data)
View(priming_data)
View(prolific_data_1)
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
sona_prolific_data = rbind(sona_data_1)
#, sona_data_2, sona_data_3, prolific_data_1)
library(dplyr)
final_data = sona_prolific_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
sona_prolific_data = rbind(sona_data_1, sona_data_2, sona_data_3)
#, prolific_data_1)
library(dplyr)
final_data = sona_prolific_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
sona_prolific_data = rbind(sona_data_1, sona_data_2, sona_data_3 , prolific_data_1)
# Chunk 3
library(dplyr)
final_data = sona_prolific_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 7
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 8
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 9
subject_level = priming_data %>%
group_by(ID, cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt))
counts = subject_level %>%
group_by(cooccurrence, relatedness) %>%
count()
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
left_join(counts) %>%
mutate(SE = sd_rt/n,
ymin = mean_rt - 1.96*SE,
ymax = mean_rt + 1.96*SE) %>%
ggplot(aes(x = cooccurrence, y = mean_rt, fill = relatedness)) +
geom_col(position = "dodge") +
geom_errorbar(aes(ymin = ymin,
ymax = ymax),
width = 0.25,
position = position_dodge(width = 0.9)) +
theme_bw() +
scale_fill_grey()+
labs(y = "Reaction Time", x = "Co-occurrence", title = "Reaction Time by Co-occurrence and Relatedness")
# Chunk 10
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
emmeans::emmeans(rt_model, pairwise ~ cooccurrence*relatedness  , adjust="tukey")
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
prolific_data_2 = read.csv("prolific_data_2.csv")
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
prolific_data_2 = read.csv("prolific_data_2.csv")
sona_prolific_data = rbind(sona_data_1, sona_data_2, sona_data_3,
prolific_data_1, prolific_data_2)
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
#prolific_data = read.csv("")
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
prolific_data_1 = read.csv("prolific_data_1.csv")
prolific_data_2 = read.csv("prolific_data_2.csv")
sona_prolific_data = rbind(sona_data_1, sona_data_2, sona_data_3,
prolific_data_1, prolific_data_2)
View(prolific_data_2)
View(prolific_data_2)
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
sona_data = rbind(sona_data_1, sona_data_2, sona_data_3)
prolific_data_1 = read.csv("prolific_data_1.csv")
prolific_data_2 = read.csv("prolific_data_2.csv")
prolific_data = rbind(prolific_data_1, prolific_data_2)
sona_prolific_data = rbind(sona_data, prolific_data)
# Chunk 3
library(dplyr)
final_data = sona_prolific_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
sona_ids = sona_data %>% pull(ID) %>% unique()
prolific_ids = prolific_data %>% pull(ID) %>% unique()
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 7
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 8
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 9
subject_level = priming_data %>%
group_by(ID, cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt))
counts = subject_level %>%
group_by(cooccurrence, relatedness) %>%
count()
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
left_join(counts) %>%
mutate(SE = sd_rt/n,
ymin = mean_rt - 1.96*SE,
ymax = mean_rt + 1.96*SE) %>%
ggplot(aes(x = cooccurrence, y = mean_rt, fill = relatedness)) +
geom_col(position = "dodge") +
geom_errorbar(aes(ymin = ymin,
ymax = ymax),
width = 0.25,
position = position_dodge(width = 0.9)) +
theme_bw() +
scale_fill_grey()+
labs(y = "Reaction Time", x = "Co-occurrence", title = "Reaction Time by Co-occurrence and Relatedness")
# Chunk 10
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
emmeans::emmeans(rt_model, pairwise ~ cooccurrence*relatedness  , adjust="tukey")
subject_level = priming_data %>%
group_by(ID, cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt))
counts = subject_level %>%
group_by(cooccurrence, relatedness) %>%
count()
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
left_join(counts) %>%
mutate(SE = sd_rt/n,
ymin = mean_rt - 1.96*SE,
ymax = mean_rt + 1.96*SE) %>%
ggplot(aes(x = cooccurrence, y = mean_rt, fill = relatedness)) +
geom_col(position = "dodge") +
geom_errorbar(aes(ymin = ymin,
ymax = ymax),
width = 0.25,
position = position_dodge(width = 0.9)) +
theme_bw() +
labs(y = "Reaction Time", x = "Co-occurrence", title = "Reaction Time by Co-occurrence and Relatedness")
# Chunk 1
library(tidyverse)
library(emmeans)
library(performance)
library(see)
library(patchwork)
library(dplyr)
library(tidyr)
library(lme4)
# Chunk 2
sona_data_1 = read.csv("sona_data_1.csv")
sona_data_2 = read.csv("sona_data_2.csv")
sona_data_3 = read.csv("sona_data_3.csv")
sona_data = rbind(sona_data_1, sona_data_2, sona_data_3)
prolific_data_1 = read.csv("prolific_data_1.csv")
prolific_data_2 = read.csv("prolific_data_2.csv")
prolific_data = rbind(prolific_data_1, prolific_data_2)
sona_prolific_data = rbind(sona_data, prolific_data)
# Chunk 3
library(dplyr)
final_data = sona_prolific_data %>%
mutate(rt = as.numeric(rt),
relatedness = as.factor(relatedness),
type = as.factor(type),
cooccurrence = as.factor(cooccurrence),
revised_correct = as.numeric(revised_correct))
# Chunk 4
nrow(final_data)
ncol(final_data)
final_data %>%
pull(ID) %>%
unique() %>%
length()
final_data %>% filter(typeoftrial == "target") %>% group_by(ID) %>% count()
#IV: cooccurrence and relatedness
#DV: rt and revised_correct
sona_ids = sona_data %>% pull(ID) %>% unique()
prolific_ids = prolific_data %>% pull(ID) %>% unique()
# Chunk 5
final_data %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct), mean_sd = sd(revised_correct))
target_data = final_data %>%
filter(typeoftrial == "target")
ggplot(data = target_data) +
geom_histogram(mapping = aes(x = rt))
target_rt = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "target")  %>%
summarise(mean_rt = mean(rt))
#revised_correct for target showing NA when trying to get mean accuracy and rt
subject_accuracy = final_data   %>%
group_by(ID)  %>%
filter(typeoftrial == "attention")  %>%
summarise(mean_accuracy = mean(revised_correct))
subject_rt = final_data   %>%
group_by(ID,typeoftrial )  %>%
filter(typeoftrial == "attention" )  %>%
summarise(mean_rt = mean(rt))
#In our 7 person study, people had quicker responses when the prime word was a related word that was trained with our target words (apple/horse)like dodish and represents a new co-occurence association. In the pre-existing co-occurence condition, surprisingly related prime words like caramelizing did not show faster reaction times compared to unrelated prime words like narrow.
# Note that novel relatedness with novel cooccurrence will be removed because that is the practice condition.
# Chunk 6
attention_trials = final_data %>% filter(typeoftrial == "attention") %>%
select(ID, revised_response, novel1, novel2, novel3, revised_correct)
## Mean
attention_trials %>%
summarize(mean_accuracy = mean(revised_correct),
sd_accuracy = sd(revised_correct))
## Summarize Participant Activity
subject_attention_accuracy = attention_trials %>%
group_by(ID) %>%
summarize(mean_accuracy = mean(revised_correct))
## Find IDs that have less than 75% accuracy
low_acc_IDs = subject_attention_accuracy %>%
filter(mean_accuracy < 0.75) %>%
pull(ID)
# Chunk 7
priming_data = final_data %>% filter(typeoftrial == "target") %>%
select(ID, rt, relatedness, prime, response, type, cooccurrence, correct, block_number, target, correct_key) %>%
filter(!is.na(rt), rt > 250, rt < 1500, correct == "TRUE", block_number == 1) %>%
filter(relatedness %in% c("related", "unrelated") & cooccurrence %in% c("novel", "preexisting")) %>%
filter(!ID %in% low_acc_IDs)
## CHANGE TYPE TO COOCCURENCE (NOVEL AND PREEXISTING)
# Chunk 8
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
ggplot() +
geom_col(mapping = aes(x= cooccurrence, y = mean_rt,
group = relatedness, fill = relatedness),
position = "dodge")+
theme_bw()+
scale_fill_grey()
# Chunk 9
subject_level = priming_data %>%
group_by(ID, cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt))
counts = subject_level %>%
group_by(cooccurrence, relatedness) %>%
count()
priming_data %>%
group_by(cooccurrence, relatedness) %>%
summarize(mean_rt = mean(rt),
sd_rt = sd(rt)) %>%
left_join(counts) %>%
mutate(SE = sd_rt/n,
ymin = mean_rt - 1.96*SE,
ymax = mean_rt + 1.96*SE) %>%
ggplot(aes(x = cooccurrence, y = mean_rt, fill = relatedness)) +
geom_col(position = "dodge") +
geom_errorbar(aes(ymin = ymin,
ymax = ymax),
width = 0.25,
position = position_dodge(width = 0.9)) +
theme_bw() +
labs(y = "Reaction Time", x = "Co-occurrence", title = "Reaction Time by Co-occurrence and Relatedness")
# Chunk 10
#Reseach Q: We will be investigating how closely related a novel co-occurring word is to the target word compared to pre-existing co-occurring related and unrelated words. We are asking if the novel co-occurring words are as integrated into our semantic network as pre-existing related words.
rt_model = lmer(data = priming_data,
rt ~ relatedness*cooccurrence + (1|ID))
summary(rt_model)
car :: Anova(rt_model)
emmeans::emmeans(rt_model, pairwise ~ cooccurrence*relatedness  , adjust="tukey")
